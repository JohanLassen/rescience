---
output: github_document
---



<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
library(knitr)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)


ggplot2::theme_set(ggplot2::theme_classic())

```

# Rescience - Setting up for Reproducible Science 

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
<!-- badges: end -->

The goal of rescience is to provide an easy framework to test different preprocessing and machine learning methods while minimizing the risk of overfitting. 

## Installation

You can install the development version of rescience from [GitHub](https://github.com/JohanLassen/rescience) with:

``` r
# install.packages("devtools")
# devtools::install_github("JohanLassen/rescience")
```

## Introduction

The package includes one dataset of untargeted metabolomics on autopsies of pneumonia deaths vs. other causes. The goal is to assist forensics teams in assessing whether suspicious deaths has happened from natural causes. To do this we can choose to use machine learning to make a predictor of pneumonia vs. control and extract the important features for model inference. Often, univariate modeling works equally well by using false discovery rates (fdr) - especially for data with less than 50 observations. This package only implements the machine learning pipeline as we observe a general need of reproducible ML (see below).

```{r example, message=F, warning=F}

library(rescience)
library(tidyverse)

# dataset
knitr::kable(head(pneumonia[,1:10]), caption = 'Feature values', align = "c")

```

To analyze this data we use the following pipeline (1) selection of important variables including outcome, batch, and technical replicates, (2) preprocessing to ensure that all features and compounds are within the same range of values and to minimize batch effect, (3) perform a machine learning model screening to evaluate which model fits the data best. Point 2 and 3 requires caution to avoid overfitting the data, and this package tries to provide the most robust setup.


# Motivation - Are my results reproducible?

We see an increasing trend in using PLS-based models in metabolomics (PLS, PLS-DA, OPLS, and OPLS-DA). As these methods are supervised (use the outcome variable), it is extremely important to perform proper validation - ideally in the form of an independent validation set but as a minimum in form of cross-validation. Many studies use validation, but we also see a trend of including the infamous "PLS scores plots" in the main text as proof of "well separation of classes". These plots are *never* representative of the actual performance when the number of observations is greatly outnumbered by the number of predictors (features).




# Running everything in your web-browser
The package is accompanied with a web app that ensures that a broad range of users can confidently use the methods. The app exists as an online version to show case the setup, but we recommend users to install R, Rstudio, and this package to experience the best performance.

To run the app:
```{r}
#run_app()
```


# Diving deeper
The true beauty of the package reveals itself when developing custom scripts and implementing your own functions. 

We strive to include as many different methods as possible to accommodate methodological studies on batch effect, machine learning algorithms, and model interpretation. Hence, it should be as easy as possible to contribute to the package by writing functions that fits the required data format.

For consistency we format data into a R list called ms. The ms list contain the feature values (ms$values) and sample meta data (ms$rowinfo).

# (1) Loading data

In the example of pneumonia we generate the ms the following way:

```{r}

# First convert the pneumonia object to a tibble. 
pneumonia <- tibble(pneumonia)

# Generate list object
ms <- list()

# Assign feature values to ms$values
start_column <- 8 # The first column with feature values
end_column <- ncol(pneumonia) # The last column of the dataset
ms$values <- pneumonia[, start_column:end_column]

# Assign metadata to ms$rowinfo
ms$rowinfo <- pneumonia %>% select(rowid = id, group, age, gender, weight, height, BMI)
```


Now the ms object is made

```{r, echo = F}

# showing first 5 columns
knitr::kable(head(ms$values[,1:5]), caption = 'Feature values', align = "c")

```

```{r, echo = F}

knitr::kable(head(ms$rowinfo), caption = 'Meta Data', align = "r")

```


# Preprocessing
```{r, fig.height=3, fig.width=6, out.width='600px', out.height="300px", fig.align='center', fig.cap='...'}

# fourth root transformation
ms <- transform_fourth_root(ms)

# Probabilistic quotient normalization
ms <- normalize_pqn(ms)

# Visualize distributions of first 10 compounds to see effect of preprocessing
ms$values[,1:5] %>% 
  pivot_longer(cols = everything(), names_to = "Feature") %>%
  mutate(Feature = as.factor(Feature)) %>% 
  ggplot(aes(x=value, fill = Feature)) +
  geom_histogram(position = "identity", alpha = 0.3)
  
```

And if we want to see the PCA plot:

```{r, fig.height=3, fig.width=10, out.width='1000px', out.height="300px", fig.align='center', fig.cap='...'}

# We plot by rowid as the data doesn't have batch info, but the rowid reflects the injection order
plot_pca(ms, color_label = "rowid") 

```

# Machine learning

## Fitting multiple models

```{r, results='hide', warning=FALSE, message=FALSE}

fits <- fit_models(x = ms$values, y=ms$rowinfo$group, methods = c("pls", "glmnet")) #PLS, Elastic net, Random Forest

```

## Obtaining the performance from the models

```{r}

performance <- get_performance(fits)

knitr::kable(performance)

```

## Plotting performance

```{r, fig.height=3, fig.width=7, out.width='700px', out.height="300px", fig.align='center', fig.cap='...'}
plot_performance(fits)
```


# Motivation part 2

## Why we should never present training data as results

To showcase that the scores plots are unreliable we can generate an OPLS model on the true data and another OPLS model on permuted (shuffled) data. In the permuted data, the features contain no signal of the permuted outcome variable (because it is now random). It is important to note that we chose OPLS, but PLS produce the same results.

> The only reproducible information reported by the PLS models is the Q2 value. This is based on cross validation of hold out data.

```{r, warning=F, message=F, }


plot_comparison <- function(ms, nobs = NULL){
  
  if (is.null(nobs)){
    nobs = nrow(ms$values)
  } 
  
  # make sampler to simulate what happens for a smaller data set
  subsample <- sample(1:nrow(ms$values), size = nobs)
  
  # prepare data
  feature_values <- ms$values[subsample,]
  outcome        <- ms$rowinfo$group[subsample]
  outcome_perm   <- sample(outcome)
  
  # Model using original data
  opls_model_true <- ropls::opls(x = feature_values, y=outcome, orthoI=1, crossvalI=5, plotI = F, fig.pdfC = "none", info.txtC = "none")
  
  # Model using permuted data
  opls_model_perm <- ropls::opls(x = feature_values, y=outcome_perm, orthoI=1, crossvalI=5, plotI = F, fig.pdfC = "none", info.txtC = "none")

  scores_orig <- tibble(t1    = opls_model_true@scoreMN %>% as.vector(),
                        to1   = opls_model_true@orthoScoreMN %>% as.vector(),
                        group = outcome 
                        )
  
  scores_perm <- tibble(t1    = opls_model_perm@scoreMN %>% as.vector(),
                        to1   = opls_model_perm@orthoScoreMN %>% as.vector(),
                        group = outcome_perm
                        )

  original <- 
    ggplot(scores_orig, aes(x=t1, y=to1, fill=outcome))+
    geom_point(shape=21, color="white")+
    stat_ellipse(geom="polygon", 
                      alpha = 0.2,
                      show.legend = FALSE, 
                      level = 0.95)+
    labs(title = "Original data", subtitle = paste("R2Y =", opls_model_true@summaryDF$`R2Y(cum)`, "and", "Q2 =", opls_model_true@summaryDF$`Q2(cum)`))
  
  permuted <-
    ggplot(scores_perm, aes(x=t1, y=to1, fill=outcome_perm))+
    geom_point(shape=21, color="white")+
    stat_ellipse(geom="polygon", 
                      alpha = 0.2,
                      show.legend = FALSE, 
                      level = 0.95)+
    labs(title = "Permuted data", subtitle = paste("R2Y =", opls_model_perm@summaryDF$`R2Y(cum)`, "and", "Q2 =", opls_model_perm@summaryDF$`Q2(cum)`))
  
  legend <- cowplot::get_legend(original + theme(legend.box.margin = margin(0, 0, 0, 12)))
  
  plots     <- cowplot::plot_grid(original+theme(legend.position = "none"), permuted+theme(legend.position = "none"), legend, ncol=3, rel_widths = c(3, 3, .6))
  summaries <- list("permuted" = opls_model_perm@summaryDF, "original" = opls_model_true@summaryDF) %>% bind_rows(.id="data")
  
  
  return(list(plots, summaries))
}


plot_comparison(ms, nobs=100)
```



